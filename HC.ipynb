{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\n",
    "Also, HC is an approach to k-means clustering for identifying groups in the dataset. HC has added advantage over K-means clustering in that it results in dendogram, which is more convenient data visualisation and simple to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two hierarchical clustering algorithms:\n",
    "\n",
    "1.Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root) (see figure below). The result is a tree which can be plotted as a dendrogram.\n",
    "\n",
    "2.Divisive hierarchical clustering: It’s also known as DIANA (Divise Analysis) and it works in a top-down manner. The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster (see figure below).Divisive hierarchical clustering is good at identifying large clusters.\n",
    "\n",
    "In our dataset I will use both agglomerative and divisive hierarchical clustering in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements for dataset in R:\n",
    "- Rows are observations (individuals) and columns are variables\n",
    "- Any missing value (NA) in the data must be removed or estimated.\n",
    "- The data must be standardized (i.e., scaled) to make variables comparable.\n",
    "\n",
    "I think that the same requirements are suitable for work in Python. But in case I am wrong, I will do HC of simulated data in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the (dis)similarity of observations using distance measures (i.e. Euclidean distance, Manhattan distance, etc.) In R, the Euclidean distance is used by default to measure the dissimilarity between each pair of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "d_{euc}(x,y) = \\sqrt{\\sum^n_{i=1}(x_i - y_i)^2} \\tag{1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the bigger question is: \n",
    "## How do we measure the dissimilarity between two clusters of observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of different cluster agglomeration methods (i.e, linkage methods) have been developed to answer to this question. The most common types methods are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Maximum or complete linkage clustering:_ It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "D(X,Y)=\\max _{{x\\in X,y\\in Y}}d(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Minimum or single linkage clustering:_ It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, “loose” clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "D(X,Y)=\\min _{{x\\in X,y\\in Y}}d(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Mean or average linkage clustering:_ It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "D_{XY} = {\\displaystyle {\\frac {1}{|X|*|Y|}}\\sum _{x\\in X}\\sum _{y\\in Y}d(x,y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Centroid linkage clustering:_ It computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "{\\displaystyle \\|c_{s}-c_{t}\\|}\\\\\n",
    "\\text {where}\\,c_{s}\\, \\text {and} \\,c_{t}\\,  \\text {are the centroids of cluster s and  t respectively}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Ward’s minimum variance method:_ It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "d_{{ij}}=d(\\{X_{i}\\},\\{X_{j}\\})={\\|X_{i}-X_{j}\\|^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation in hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendograms are mostly used in Hierarchical Clustering.\n",
    "The dendrogram is a visual representation of the compound correlation data. The individual compounds are arranged along the bottom of the dendrogram and referred to as leaf nodes. Compound clusters are formed by joining individual compounds or existing compound clusters with the join point referred to as a node.\n",
    "In my dataset example I will use dendograms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros of Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well whereas with other clustering algorithms, the choice of distance metric is critical. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can’t do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons of Hierarchical Clusstering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using hierarchical clustering it is necessary to specify both the distance metric and the linkage criteria. There is rarely any strong theoretical basis for such decisions. A core principle of science is that findings are not the result of arbitrary decisions, which makes the technique of dubious relevance in modern research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most hierarchical clustering software does not work with values are missing in the data. As in my example of data clustering in R, I had to clean all the missing data and ect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many users believe that such dendrograms can be used to select the number of clusters. However, this is true only when the ultrametric tree inequality holds, which is rarely, if ever, the case in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of cluster agglomeration methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Maximum or complete linkage clustering:_ it's very expensive computationally;\n",
    "- _Minimum or single linkage clustering:_ encourages chaining: similarity is usually not transitive:i.e. if A is similar to B, and B is similar to C, it doesn't mean that A must be similar to C;\n",
    "- _Mean or average linkage clustering:_ It's way slower than single linkage because requires a lot of computations\n",
    "- _Centroid linkage clustering:_ can produce dendograms with inversions, provides no interpretation for the clusters resulting from cutting the tree\n",
    "- _Ward’s minimum variance method:_ sensitive to data noise, has difficulties with clusters of unequal diameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my dataset clustering I will use complete, single, Ward's minimum and average agglomeration methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R I will use complete dataset for deeper analysis of hierarchical clustering.\n",
    "In Phyton I will stimulate data just to see how it works using different methods and what diferences there were among them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data examples:\n",
    "\n",
    "One of the famous datasets Iris data shows how R package can be used to enhance HC.\n",
    "\n",
    "https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html\n",
    "\n",
    "https://uc-r.github.io/hc_clustering My interpretation of datset clustering was based on this example.(R)\n",
    "\n",
    "Fr Phyton clustering I took this as basis: https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "https://en.wikipedia.org/wiki/Complete-linkage_clustering, \n",
    "\n",
    "https://www.displayr.com/strengths-weaknesses-hierarchical-clustering/\n",
    "\n",
    "https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/\n",
    "\n",
    "https://www.quora.com/What-are-the-pros-and-cons-of-the-Wards-method-in-hierarchical-cluster-analysis\n",
    "\n",
    "https://uc-r.github.io/hc_clustering\n",
    "\n",
    "http://www.sthda.com/english/wiki/print.php?id=237\n",
    "\n",
    "http://mlwiki.org/index.php/Agglomerative_Clustering\n",
    "\n",
    "https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n",
    "        \n",
    "https://afit-r.github.io/hc_clustering\n",
    "\n",
    "https://uc-r.github.io/kmeans_clustering\n",
    "\n",
    "https://rpubs.com/JanpuHou/278558\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/hierarchical-clustering-R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
